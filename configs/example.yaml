# Example configuration for gollmperf

# Test configuration
test:
  # Duration of the test, 0 means infinite
  duration: 60s
  
  # Warmup period before starting measurements
  warmup: 10s
  
  # Number of concurrent requests, used for batch mode and stress mode
  concurrency: 10

  # Performance concurrency group, only used for perf mode
  perf_concurrency_group: [1,2,4,8,16,24,32,48,64]

  # Number of requests per concurrency level, 0 means infinite
  requests_per_concurrency: 100
  
  # Timeout for each request, 0 means infinite
  timeout: 30s

# Model configuration
model:
  # Model name
  name: ${LLM_MODEL_NAME}
  
  # Provider (openai, qwen, etc.)
  provider: openai
  
  # API endpoint (optional, uses default if not specified)
  endpoint: ${LLM_API_ENDPOINT}

  # API key (required)
  api_key: ${LLM_API_KEY}

  # http headers, with any additional header fields
  headers:
    Content-Type: application/json

  # http request params template, with any additional fields
  params_template:
    stream: true
    stream_options:
      include_usage: true
    extra_body:
      enable_thinking: false

  # system prompt template
  # Supports two formats:
  # 1. Direct content (using content field)
  # 2. File path (using path field, content takes precedence if both are specified)
  system_prompt_template:
    enable: false
    content: |
      You are a helpful assistant.
    path: ./examples/system_prompt.md

# Dataset configuration
dataset:
  # Type of dataset (jsonl)
  type: jsonl
  
  # Path to dataset file
  path: ./examples/test_cases.jsonl

# Output configuration
output:
  # Output report file format (json, csv, html)
  format: html

  # Output report file path
  path: ./results/report.html

  # Batch results file path
  batch_result_path: ./results/batch_results.jsonl

